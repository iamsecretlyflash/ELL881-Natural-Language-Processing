{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ngram import Ngram\n",
    "import numpy as np \n",
    "import math\n",
    "unigram = Ngram(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = sorted(unigram.train_vocab.items(),key = lambda x : x[1]) #Vocbulary of the data present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_counts(vocab):\n",
    "    #To calculate Nc and to store words with c counts\n",
    "    counter = defaultdict(lambda : 0)# stores Nc\n",
    "    count_words = defaultdict(lambda : []) #words occuring c times\n",
    "    for j in range(len(vocab)):\n",
    "        counter[vocab[j][1]]+=1\n",
    "        count_words[vocab[j][1]].append(vocab[j][0])\n",
    "    return counter,count_words\n",
    "counter,count_words = get_counts(train_vocab) \n",
    "N = sum(unigram.train_vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_available = list(counter.keys())#Total unique number of occurences of words\n",
    "len(counts_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probGT(counter,counts_avaliable,N,c):\n",
    "        if c==0:\n",
    "            #probability for unseen words\n",
    "            return counter[1]/N\n",
    "        Nc = counter[c]\n",
    "        try:\n",
    "         Nc_1 =counter[counts_available[binary_search(c)+1]]#to calculate Nc+1\n",
    "         return (c+1)*Nc_1/(Nc*N)\n",
    "        except :\n",
    "            prob = 1\n",
    "            temp = [0]+counts_available[:-1]\n",
    "            for i in temp:\n",
    "             mult = counter[i]\n",
    "             if i == 0 :\n",
    "                mult = 1\n",
    "             prob-= mult * probGT(counter=counter,counts_avaliable=counts_available,N=18,c=i)\n",
    "             # 1-Probability of other counts\n",
    "            return prob\n",
    "\n",
    "def binary_search(x):\n",
    "    arr = counts_available\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid] < x:\n",
    "            low = mid + 1\n",
    "        elif arr[mid] > x:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS NOT PRESENT IN TRAINING VOCAB\n",
      "represents\n",
      "imbued\n",
      "okay’\n",
      "\n",
      "Generated Text : \n",
      "streaming priority mainly represents disappear imbued facing pang scrabbled glinted digging notify bringing drift nagging safest glittered screams teddy identity dawn sugar embarrassed glances bundles powerful underwear hover farther fought knowing picked cruelty oppression okay’ amusement treading raving it’d halting\n",
      "\n",
      "Perplexity : 63122.916973741856\n"
     ]
    }
   ],
   "source": [
    "def generategt(text_len,vocab):\n",
    "        res = [];i = 0;perp=0;N = sum(unigram.train_vocab.values())\n",
    "        while i<text_len:\n",
    "            word_probs= {}\n",
    "            for wrd in vocab:\n",
    "                word_probs[wrd]=probGT(counter,counts_available,N,unigram.train_vocab[wrd])\n",
    "            wrd = np.random.choice(list(word_probs.keys()),1,list(word_probs.values()))[0]\n",
    "            if unigram.train_vocab[wrd]==0:print(wrd)\n",
    "            if wrd!='</s>':\n",
    "                perp+=math.log(word_probs[wrd])\n",
    "                res.append(wrd)\n",
    "                i+=1\n",
    "            else:\n",
    "                res.append('.')\n",
    "        perp = math.exp(abs(perp/len(res)))\n",
    "        return ' '.join(res),perp\n",
    "print(\"\\nWORDS NOT PRESENT IN TRAINING VOCAB\")\n",
    "text,perp = generategt(40,unigram.test_vocab)\n",
    "print(f\"\\nGenerated Text : \\n{text}\\n\\nPerplexity : {perp}\")\n",
    "\n",
    "def perplexity(sent,model = unigram):\n",
    "        N = sum(unigram.train_vocab.values())\n",
    "        if type(sent) == str:\n",
    "            sent = sent.split()\n",
    "        perp=0\n",
    "        for wrd in sent:\n",
    "            if wrd == '.':\n",
    "                wrd = '</s>'\n",
    "            try:\n",
    "                perp+=math.log(probGT(counter,counts_available,N,model.train_vocab[wrd]))\n",
    "            except:\n",
    "                    perp+=0\n",
    "        perp = math.exp(abs(perp/len(sent)))\n",
    "        return perp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you will present it as your own idea </s>\n",
      "372.00469506447706\n",
      "he strode around the table and hugged harry the scene in the basement of grimmauld place might never have happened </s>\n",
      "545.0712672273492\n",
      "perfect panted hermione </s>\n",
      "563.1861012679765\n",
      "it seemed that he forced himself to meet harry’s eyes </s>\n",
      "299.2218968785525\n",
      "</s>\n",
      "1.0\n",
      "how — how’re we going to get in panted ron </s>\n",
      "381.96355557625526\n",
      "i say that’s potter and him plus his wand that’s two hundred thousand galleons right there but if you’re too gutless to come along any of you it’s all for me and with any luck i’ll get the girl thrown in — the window was the merest slit in the black rock not big enough for a man to enter </s>\n",
      "602.9830982335187\n",
      "it is a miracle you managed to return here snape sounded furious </s>\n",
      "589.834847010513\n",
      "fear and panic were clouding his thought processes </s>\n",
      "626.8461818689724\n",
      "a swarm of dementors was gliding amongst the trees he could feel their chill and he was not sure he would be able to pass safely through it </s>\n",
      "620.2368731943643\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    rand = np.random.randint(0,len(unigram.test),10)\n",
    "    for i in rand:\n",
    "        print(' '.join(unigram.test[i]))\n",
    "        print(perplexity(unigram.test[i]))\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of generating sentences using test set vocabulary is quite high as expected because this is essentially a unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007131434358560426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probGT(counter,counts_available,N,unigram.train_vocab['cruciate'])\n",
    "#Probability for unseen words : 0.007131434358560426"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.vocab['</s>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'picture',\n",
       " 'hermione',\n",
       " 'it’s',\n",
       " 'the',\n",
       " 'thief',\n",
       " 'the',\n",
       " 'thief',\n",
       " 'who',\n",
       " 'stole',\n",
       " 'from',\n",
       " 'gregorovitch',\n",
       " 'please',\n",
       " 'he',\n",
       " 'said',\n",
       " 'to',\n",
       " 'bathilda',\n",
       " '</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram.test[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
