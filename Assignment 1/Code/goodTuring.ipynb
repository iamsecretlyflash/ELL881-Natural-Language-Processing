{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Ngram import Ngram\n",
    "import numpy as np \n",
    "import math\n",
    "unigram = Ngram(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = sorted(unigram.train_vocab.items(),key = lambda x : x[1]) #Vocbulary of the data present in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_counts(vocab):\n",
    "    #To calculate Nc and to store words with c conunts\n",
    "    counter = defaultdict(lambda : 0)# stores Nc\n",
    "    count_words = defaultdict(lambda : []) #words occuring c times\n",
    "    for j in range(len(vocab)):\n",
    "        counter[vocab[j][1]]+=1\n",
    "        count_words[vocab[j][1]].append(vocab[j][0])\n",
    "    return counter,count_words\n",
    "counter,count_words = get_counts(train_vocab) \n",
    "N = sum(unigram.train_vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_available = list(counter.keys())#Total unique number of occurences of words\n",
    "len(counts_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probGT(counter,counts_avaliable,N,c):\n",
    "        if c==0:\n",
    "            #probability for unseen words\n",
    "            return counter[1]/N\n",
    "        Nc = counter[c]\n",
    "        try:\n",
    "         Nc_1 =counter[counts_available[binary_search(c)+1]]#to calculate Nc+1\n",
    "         return (c+1)*Nc_1/(Nc*N)\n",
    "        except :\n",
    "            prob = 1\n",
    "            temp = [0]+counts_available[:-1]\n",
    "            for i in temp:\n",
    "             mult = counter[i]\n",
    "             if i == 0 :\n",
    "                mult = 1\n",
    "             prob-= mult * probGT(counter=counter,counts_avaliable=counts_available,N=18,c=i)\n",
    "             # 1-Probability of other counts\n",
    "            return prob\n",
    "\n",
    "def binary_search(x):\n",
    "    arr = counts_available\n",
    "    low = 0\n",
    "    high = len(arr) - 1\n",
    "    mid = 0\n",
    "    while low <= high:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid] < x:\n",
    "            low = mid + 1\n",
    "        elif arr[mid] > x:\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            return mid\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS NOT PRESENT IN TRAINING VOCAB\n",
      "swigged\n",
      "’ermione\n",
      "conspicuously\n",
      "authenticity\n",
      "clankers\n",
      "emphasized\n",
      "\n",
      "Generated Text : \n",
      "offensive undercover imprisoned tubers swigged identify willow’s tramping ’ermione grinned tighten conspicuously soften author sitting camping creature’s disapparated authenticity refuge regrow clankers emphasized chief shaped insist wizengamot head entwined suffocating emergency wriggling weapon working tales members potted thoughts tied indigo\n",
      "\n",
      "Perplexity : 43662.30779300691\n"
     ]
    }
   ],
   "source": [
    "def generategt(text_len,vocab):\n",
    "        res = [];i = 0;perp=0;N = sum(unigram.train_vocab.values())\n",
    "        while i<text_len:\n",
    "            word_probs= {}\n",
    "            for wrd in vocab:\n",
    "                word_probs[wrd]=probGT(counter,counts_available,N,unigram.train_vocab[wrd])\n",
    "            wrd = np.random.choice(list(word_probs.keys()),1,list(word_probs.values()))[0]\n",
    "            if unigram.train_vocab[wrd]==0:print(wrd)\n",
    "            if wrd!='</s>':\n",
    "                perp+=math.log(word_probs[wrd])\n",
    "                res.append(wrd)\n",
    "                i+=1\n",
    "            else:\n",
    "                res.append('.')\n",
    "        perp = math.exp(abs(perp/len(res)))\n",
    "        return ' '.join(res),perp\n",
    "print(\"\\nWORDS NOT PRESENT IN TRAINING VOCAB\")\n",
    "text,perp = generategt(40,unigram.test_vocab)\n",
    "print(f\"\\nGenerated Text : \\n{text}\\n\\nPerplexity : {perp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity of generating sentences using test set vocabulary is quite high as expected because this is essentially a unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007131434358560426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probGT(counter,counts_available,N,unigram.train_vocab['cruciate'])\n",
    "#Probability for unseen words : 0.007131434358560426"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
