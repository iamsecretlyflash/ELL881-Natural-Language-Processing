{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-05-06T16:42:04.128517Z","iopub.status.busy":"2023-05-06T16:42:04.126208Z","iopub.status.idle":"2023-05-06T16:42:08.113920Z","shell.execute_reply":"2023-05-06T16:42:08.112769Z","shell.execute_reply.started":"2023-05-06T16:42:04.128483Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#CODE COPIED FROM LCS2-IIITD DABERTA-EMNLP-2022 Repository\n","import math\n","import copy\n","from typing import Optional, List\n","import torch\n","from torch import nn\n","\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, num_layers, num_heads, dim_feedforward=2048):\n","        super(TransformerEncoder, self).__init__()\n","        self.d_model = d_model\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dim_feedforward)\n","        self.encoder = _TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.pos_encoder = PositionalEncoding(d_model, dropout=0.1)\n","\n","    def forward(self, inputs: torch.Tensor, lens: Optional[List[int]] = None):\n","        if lens is not None:\n","            max_len = max(lens)\n","\n","            mask = [([False] * l + [True] * (max_len - l)) for l in lens]\n","            mask = torch.tensor(mask).to(device=inputs.device)\n","        else:\n","            mask = None\n","\n","        inputs = inputs.permute(1, 0, 2)\n","\n","        inputs = inputs * math.sqrt(self.d_model)\n","        inputs = self.pos_encoder(inputs)\n","\n","        outputs = self.encoder(src=inputs, src_key_padding_mask=mask) # (seq_len, bs, dim)\n","\n","        return [o.permute(1, 0, 2) for o in outputs]\n","\n","\n","    \n","def padTensor(t: torch.Tensor, targetLen: int) -> torch.Tensor:\n","    oriLen, dim = t.size()\n","    return torch.cat((t, torch.zeros(targetLen - oriLen, dim).to(t.device)), dim=0)\n","\n","\n","\n","def _get_clones(module, N):\n","    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n","\n","\n","\n","class _TransformerEncoder(nn.Module):\n","    def __init__(self, encoder_layer, num_layers, norm=None):\n","        super(_TransformerEncoder, self).__init__()\n","        self.layers = _get_clones(encoder_layer, num_layers)\n","        self.num_layers = num_layers\n","        self.norm = norm\n","\n","    def forward(self, src: torch.Tensor, mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        outputs = [src]\n","\n","        for mod in self.layers:\n","            output = mod(outputs[-1], src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n","            outputs.append(output)\n","\n","        if self.norm is not None:\n","            outputs[-1] = self.norm(outputs[-1])\n","\n","        return outputs[1:]\n","\n","    \n","    \n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:42:37.290864Z","iopub.status.busy":"2023-05-06T16:42:37.290482Z","iopub.status.idle":"2023-05-06T16:42:44.389830Z","shell.execute_reply":"2023-05-06T16:42:44.388872Z","shell.execute_reply.started":"2023-05-06T16:42:37.290826Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU\n","Seed: 0\n"]}],"source":["# Code for Description Aware Roberta (DABERTa)\n","# File: daberta.py\n","#CODE COPIED FROM LCS2-IIITD DABERTA-EMNLP-2022 Repository\n","# ----------------------------------------------------- IMPORTS -----------------------------------------------------\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import json\n","import warnings\n","import logging\n","import gc\n","import random\n","import math\n","import re\n","import ast\n","import wandb\n","from tqdm import tqdm\n","from typing import Optional\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","\n","from datetime import datetime\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchcrf import CRF\n","\n","from torch.utils.data import (\n","    DataLoader, \n","    TensorDataset\n",")\n","\n","from torch.cuda.amp import (\n","    autocast, \n","    GradScaler\n",")\n","\n","from transformers.modeling_utils import (\n","    PreTrainedModel, \n","    unwrap_model\n",")\n","\n","from transformers import (\n","    RobertaTokenizerFast,\n","    AdamW\n",")\n","\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda\")\n","    print(\"Using GPU\")\n","else:\n","    DEVICE = torch.device(\"cpu\")\n","    print(\"Using CPU\")\n","    \n","    \n","SCALER = GradScaler()\n","\n","\n","# ----------------------------------------------------- CONFIGS -----------------------------------------------------\n","\n","INPUT_PATH = 'dataset/'\n","OUTPUT_PATH = 'models/'\n","\n","TWEET_MAX_LEN = 185\n","DEFINITION_MAX_LEN = 25\n","\n","LABEL_ALL_TOKENS = True\n","PAD_TO_MAX_LENGTH = True\n","TRUNCATION = True\n","\n","BATCH_SIZE = 16\n","MAX_EPOCHS = 5\n","\n","BASE_LEARNING_RATE = 4e-5\n","\n","CLAIM_DEFINITIONS_LEN = 18\n","\n","\n","def set_random_seed(seed: int):\n","    \"\"\"\n","    Helper function to seed experiment for reproducibility.\n","    If -1 is provided as seed, experiment uses random seed from 0~9999\n","    Args:\n","        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n","    \"\"\"\n","    print(\"Seed: {}\".format(seed))\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.enabled = False\n","    torch.backends.cudnn.deterministic = True\n","\n","    random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    \n","SEED = 0\n","set_random_seed(SEED)\n","\n","# ----------------------------------------------------- MODEL ARCHITECTURE -----------------------------------------------------\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from typing import Optional, Tuple\n","\n","from transformers.models.roberta.configuration_roberta import RobertaConfig\n","\n","from transformers.models.roberta.modeling_roberta import (\n","    RobertaPreTrainedModel,\n","    RobertaLayer,\n","    RobertaEmbeddings,\n","    RobertaPooler\n",")\n","\n","from transformers.modeling_outputs import (\n","    BaseModelOutputWithPastAndCrossAttentions,\n","    BaseModelOutputWithPoolingAndCrossAttentions,\n","    TokenClassifierOutput\n",")\n","\n","\n","# ----------------------------------------------------- IGM MODULE -----------------------------------------------------\n","\n","class IGM(nn.Module):\n","\n","    def __init__(self,\n","                 dim: int):\n","        super(IGM, self).__init__()\n","\n","        # Conflict Gate\n","        self.w_c1 = nn.Linear(dim, dim, bias=False)\n","        self.w_c2 = nn.Linear(dim, dim, bias=False)\n","        self.b_mu_c = nn.Parameter(torch.rand(dim))\n","        torch.nn.init.uniform_(self.b_mu_c, a=-math.sqrt(dim), b=math.sqrt(dim))\n","\n","        self.w_c3 = nn.Linear(dim, dim, bias=False)\n","        self.w_c4 = nn.Linear(dim, dim, bias=False)\n","        self.b_c = nn.Parameter(torch.rand(dim))\n","        torch.nn.init.uniform_(self.b_c, a=-math.sqrt(dim), b=math.sqrt(dim))\n","\n","        # Refine Gate\n","        self.w_r1 = nn.Linear(dim, dim, bias=False)\n","        self.w_r2 = nn.Linear(dim, dim, bias=False)\n","        self.b_mu_r = nn.Parameter(torch.rand(dim))\n","        torch.nn.init.uniform_(self.b_mu_r, a=-math.sqrt(dim), b=math.sqrt(dim))\n","\n","        self.w_r3 = nn.Linear(dim, dim, bias=False)\n","        self.w_r4 = nn.Linear(dim, dim, bias=False)\n","        self.b_r = nn.Parameter(torch.rand(dim))\n","        torch.nn.init.uniform_(self.b_r, a=-math.sqrt(dim), b=math.sqrt(dim))\n","\n","        # Adaptive Gate\n","        self.w_a =  nn.Linear(dim, dim, bias=False)\n","        self.b_a = nn.Parameter(torch.rand(dim))\n","        torch.nn.init.uniform_(self.b_a, a=-math.sqrt(dim), b=math.sqrt(dim))\n","\n","\n","\n","\n","    def forward(self, \n","                input1: torch.tensor,\n","                input2: torch.tensor):\n","        # MAx Pooling\n","        pooled_input1, _ = torch.max(input1, dim=1)        \n","        pooled_input2, _ = torch.max(input2, dim=1)\n","        \n","        # Conflict Gate\n","        mu_c = F.sigmoid(self.w_c1(pooled_input1) + self.w_c2(pooled_input2) + self.b_mu_c)\n","        conflict = F.tanh(self.w_c3(torch.mul(mu_c, pooled_input1)) + self.w_c4(torch.mul((1 - mu_c), pooled_input2)) + self.b_c)\n","        \n","        # Refine Gate\n","        mu_r = F.sigmoid(self.w_r1(pooled_input1) + self.w_r2(pooled_input2) + self.b_mu_r)\n","        refine = F.tanh(self.w_r3(torch.mul(mu_r, pooled_input1)) + self.w_r4(torch.mul(mu_r, pooled_input2)) + self.b_r)\n","        \n","        # Adaptive Gate\n","        adapt = refine + torch.mul((1 - mu_r), conflict)\n","        interact = F.tanh(self.w_a(adapt) + self.b_a)   \n","        interact = torch.unsqueeze(interact, 1).repeat(1, input1.shape[1], 1)        \n","        output = torch.mul(interact, input1)\n","        return output\n","    \n","# ----------------------------------------------------- CoDA MODULE -----------------------------------------------------\n","\n","class CoDA(nn.Module):\n","\n","    def __init__(self,\n","                 dim_model: int,\n","                 multi_head: Optional[bool]=False,\n","                 num_heads: Optional[int]=1,\n","                 alpha: Optional[float]=1.0,\n","                 beta: Optional[float]=1.0,\n","                 scaling: Optional[bool]=False,\n","                 centering_E: Optional[bool]=False,\n","                 centering_N: Optional[bool]=False):\n","        super(CoDA, self).__init__()\n","        \n","        assert (\n","            dim_model % num_heads == 0\n","        ), f\"dim_model must be divisible by num_heads (got `embed_dim`: {dim_model} and `num_heads`: {num_heads}).\"\n","\n","        self.dim_model = dim_model\n","        self.multi_head = multi_head\n","        self.num_heads = num_heads\n","        \n","        if self.multi_head and self.num_heads > 1:\n","            self.head_dim = self.dim_model // self.num_heads\n","            self.query_transform = nn.Linear(self.dim_model, self.num_heads * self.head_dim, bias=False)\n","            self.key_transform = nn.Linear(self.dim_model, self.num_heads * self.head_dim, bias=False)\n","            self.value_transform = nn.Linear(self.dim_model, self.num_heads * self.head_dim, bias=False)\n","        \n","        else:\n","            self.query_transform = nn.Linear(self.dim_model, self.dim_model, bias=False)\n","            self.key_transform = nn.Linear(self.dim_model, self.dim_model, bias=False)\n","            self.value_transform = nn.Linear(self.dim_model, self.dim_model, bias=False)\n","\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.scaling = scaling\n","        self.centering_E = centering_E\n","        self.centering_N = centering_N\n","        \n","        self.fc = nn.Linear(self.dim_model, self.dim_model)\n","        self.dropout = nn.Dropout(0.2)\n","        self.layer_norm = nn.LayerNorm(self.dim_model)\n","      \n","\n","\n","\n","    \n","    def coda_attention(self,\n","                       q: torch.Tensor, \n","                       k: torch.Tensor,\n","                       v: torch.Tensor):  \n","        # KEY AND VALUE ARE FROM THE DIFFERENT MODALITY\n","        # QUERY REMAINS THE SAME\n","        if self.multi_head and self.num_heads > 1:\n","            E = torch.mul(self.alpha, torch.matmul(q, k.transpose(-2, -1)))\n","            if self.centering_E:\n","                E = E - torch.mean(E)\n","\n","            N = None\n","            q = q.permute(1, 0, 2, 3)\n","            k = k.permute(1, 0, 2, 3)\n","            for head_q, head_k in list(zip(q, k)):\n","                head_N = torch.cdist(head_q, head_k, p=1)\n","\n","                head_N = head_N.unsqueeze(0)\n","                if N is None:\n","                    N = head_N\n","                else:\n","                    N = torch.cat([N, head_N], dim=0)\n","\n","            q = q.permute(1, 0, 2, 3)\n","            k = k.permute(1, 0, 2, 3)\n","            N = N.permute(1, 0, 2, 3)\n","            \n","            if self.centering_N:\n","                N = N - torch.mean(N)\n","\n","            if self.scaling:\n","                E  = E / math.sqrt(k.shape[-1])\n","                N = N / math.sqrt(k.shape[-1])\n","\n","            if self.centering_N:\n","                coda = torch.mul(F.tanh(E), F.sigmoid(N))\n","            else:\n","                coda = torch.mul(F.tanh(E), F.sigmoid(N))\n","            output = torch.matmul(coda, v)\n","            return output\n","        \n","        else:\n","            E = torch.mul(self.alpha, torch.matmul(q, k.transpose(-2, -1)))\n","            if self.centering_E:\n","                E = E - torch.mean(E)\n","\n","            N = torch.mul(-self.beta, torch.cdist(q, k, p=1))\n","            if self.centering_N:\n","                N = N - torch.mean(N)\n","\n","            if self.scaling:\n","                E  = E / math.sqrt(k.shape[-1])\n","                N = N / math.sqrt(k.shape[-1])\n","\n","            if self.centering_N:\n","                coda = torch.mul(F.tanh(E), F.sigmoid(N))\n","            else:\n","                coda = torch.mul(F.tanh(E), F.sigmoid(N))\n","            output = torch.matmul(coda, v)\n","            return output\n","\n","    \n","\n","    def forward(self, \n","                query: torch.Tensor, \n","                key: torch.Tensor,\n","                value: torch.Tensor):\n","        \n","        batch_size = query.shape[0]\n","        residual = query\n","        \n","        query = self.query_transform(query)\n","        key = self.key_transform(key)\n","        value = self.value_transform(value)\n","\n","        if self.multi_head and self.num_heads > 1:\n","            query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","            key = key.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","            value = value.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n","            coda_output = self.coda_attention(q=query,\n","                                              k=key,\n","                                              v=value)\n","            coda_output = coda_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dim_model)\n","        else:\n","            coda_output = self.coda_attention(q=query,\n","                                              k=key,\n","                                              v=value)\n","        coda_output = self.fc(self.dropout(coda_output))\n","        coda_output = self.layer_norm(coda_output + residual)\n","        return coda_output\n","    \n","    \n","# ----------------------------------------------------- DESCNET MODULE -----------------------------------------------------\n","\n","class DescNet(nn.Module):\n","\n","    def __init__(self, \n","                 dim_model: int,\n","                 multi_head: Optional[bool]=False,\n","                 num_heads: Optional[int]=1,\n","                 alpha: Optional[float]=1.0,\n","                 beta: Optional[float]=1.0,\n","                 scaling: Optional[bool]=False,\n","                 centering_E: Optional[bool]=False,\n","                 centering_N: Optional[bool]=False):\n","        super(DescNet, self).__init__()\n","        self.def_encoder = TransformerEncoder(d_model=dim_model, \n","                                              num_layers=2,\n","                                              num_heads=4, \n","                                              dim_feedforward=dim_model)\n","        self.attention_layer = CoDA(dim_model=dim_model,\n","                                    multi_head=multi_head,\n","                                    num_heads=num_heads,\n","                                    alpha=alpha,\n","                                    beta=beta,\n","                                    scaling=scaling,\n","                                    centering_E=centering_E,\n","                                    centering_N=centering_N)\n","        self.igm_layer = IGM(dim=dim_model)\n","        self.fc = nn.Linear(CLAIM_DEFINITIONS_LEN * dim_model, dim_model)\n","        self.dropout_1 = nn.Dropout(0.2)\n","        self.dropout_2 = nn.Dropout(0.2)\n","        \n","        \n","\n","\n","\n","    def forward(self, \n","                encoder_output: torch.tensor,\n","                definition_inputs: torch.tensor):   \n","        defnet_out = None\n","        definition_inputs = definition_inputs.permute(1, 0, 2, 3)\n","        for definition_input in definition_inputs:\n","            definition_input = self.def_encoder(definition_input)[-1]\n","            attention_out = self.attention_layer(query=encoder_output, \n","                                                 key=definition_input,\n","                                                 value=definition_input)\n","            if defnet_out is None:\n","                defnet_out = attention_out\n","            else:\n","                defnet_out = torch.cat([defnet_out, attention_out], dim=-1)\n","\n","        output = self.fc(defnet_out)\n","        output = self.dropout_1(output)\n","        output = self.igm_layer(encoder_output, output)\n","        output = self.dropout_2(output)\n","        return output   \n","    \n","    \n","# ----------------------------------------------------- ROBERTA ENCODER -----------------------------------------------------\n","\n","class CustomRobertaEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n","        self.gradient_checkpointing = False\n","        \n","        # ==================================== Modifications ==================================== #\n","        self.fusion_at_layer = 11\n","        self.desc_net = DescNet(dim_model=config.hidden_size,\n","                                multi_head=False,\n","                                num_heads=1,\n","                                alpha=1.0,\n","                                beta=1.0,\n","                                scaling=True,\n","                                centering_E=False,\n","                                centering_N=False)\n","        # ======================================================================================= #\n","        \n","        \n","        \n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        definition_inputs=None,      # New addition of definition_inputs\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=False,\n","        output_hidden_states=False,\n","        return_dict=True,\n","    ):\n","        all_hidden_states = () if output_hidden_states else None\n","        all_self_attentions = () if output_attentions else None\n","        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n","\n","        next_decoder_cache = () if use_cache else None\n","        for i, layer_module in enumerate(self.layer):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_head_mask = head_mask[i] if head_mask is not None else None\n","            past_key_value = past_key_values[i] if past_key_values is not None else None\n","\n","            if self.gradient_checkpointing and self.training:\n","\n","                if use_cache:\n","                    logger.warning(\n","                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n","                    )\n","                    use_cache = False\n","\n","                def create_custom_forward(module):\n","                    def custom_forward(*inputs):\n","                        return module(*inputs, past_key_value, output_attentions)\n","\n","                    return custom_forward\n","\n","                layer_outputs = torch.utils.checkpoint.checkpoint(\n","                    create_custom_forward(layer_module),\n","                    hidden_states,\n","                    attention_mask,\n","                    layer_head_mask,\n","                    encoder_hidden_states,\n","                    encoder_attention_mask,\n","                )\n","            else:\n","                layer_outputs = layer_module(\n","                    hidden_states,\n","                    attention_mask,\n","                    layer_head_mask,\n","                    encoder_hidden_states,\n","                    encoder_attention_mask,\n","                    past_key_value,\n","                    output_attentions,\n","                )\n","\n","            hidden_states = layer_outputs[0]\n","            \n","            # ==================================== Modifications ==================================== #\n","            if i == self.fusion_at_layer:\n","                hidden_states = self.desc_net(encoder_output=hidden_states,\n","                                              definition_inputs=definition_inputs)\n","            # ======================================================================================= #\n","            \n","            if use_cache:\n","                next_decoder_cache += (layer_outputs[-1],)\n","            if output_attentions:\n","                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n","                if self.config.add_cross_attention:\n","                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n","\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(\n","                v\n","                for v in [\n","                    hidden_states,\n","                    next_decoder_cache,\n","                    all_hidden_states,\n","                    all_self_attentions,\n","                    all_cross_attentions,\n","                ]\n","                if v is not None\n","            )\n","        return BaseModelOutputWithPastAndCrossAttentions(\n","            last_hidden_state=hidden_states,\n","            past_key_values=next_decoder_cache,\n","            hidden_states=all_hidden_states,\n","            attentions=all_self_attentions,\n","            cross_attentions=all_cross_attentions,\n","        )\n","\n","\n","# Copied from transformers.models.bert.modeling_bert.BertPooler\n","class RobertaPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","    \n","# ----------------------------------------------------- ROBERTA MODEL -----------------------------------------------------\n","\n","class CustomRobertaModel(RobertaPreTrainedModel):\n","    \"\"\"\n","\n","    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n","    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n","    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n","    Kaiser and Illia Polosukhin.\n","\n","    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n","    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n","    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n","    input to the forward pass.\n","\n","    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n","\n","    \"\"\"\n","\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n","    def __init__(self, config, add_pooling_layer=True):\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.embeddings = RobertaEmbeddings(config)\n","        self.encoder = CustomRobertaEncoder(config)\n","\n","        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n","\n","        self.init_weights()\n","        \n","\n","    def get_input_embeddings(self):\n","        return self.embeddings.word_embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        definition_inputs=None,      # New addition of definition_inputs\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n","            the model is configured as a decoder.\n","        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n","            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n","\n","            - 1 for tokens that are **not masked**,\n","            - 0 for tokens that are **masked**.\n","        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n","            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n","\n","            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n","            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n","            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n","        use_cache (:obj:`bool`, `optional`):\n","            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n","            decoding (see :obj:`past_key_values`).\n","        \"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if self.config.is_decoder:\n","            use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        else:\n","            use_cache = False\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = input_ids.size()\n","        elif inputs_embeds is not None:\n","            input_shape = inputs_embeds.size()[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        batch_size, seq_length = input_shape\n","        device = input_ids.device if input_ids is not None else inputs_embeds.device\n","\n","        # past_key_values_length\n","        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n","\n","        if attention_mask is None:\n","            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n","\n","        if token_type_ids is None:\n","            if hasattr(self.embeddings, \"token_type_ids\"):\n","                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n","                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n","                token_type_ids = buffered_token_type_ids_expanded\n","            else:\n","                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n","\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n","\n","        # If a 2D or 3D attention mask is provided for the cross-attention\n","        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","        if self.config.is_decoder and encoder_hidden_states is not None:\n","            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n","            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n","            if encoder_attention_mask is None:\n","                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n","            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n","        else:\n","            encoder_extended_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n","\n","        embedding_output = self.embeddings(\n","            input_ids=input_ids,\n","            position_ids=position_ids,\n","            token_type_ids=token_type_ids,\n","            inputs_embeds=inputs_embeds,\n","            past_key_values_length=past_key_values_length,\n","        )\n","        \n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            attention_mask=extended_attention_mask,\n","            definition_inputs=definition_inputs,      # New addition of definition_inputs\n","            head_mask=head_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_extended_attention_mask,\n","            past_key_values=past_key_values,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n","\n","        if not return_dict:\n","            return (sequence_output, pooled_output) + encoder_outputs[1:]\n","\n","        return BaseModelOutputWithPoolingAndCrossAttentions(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            past_key_values=encoder_outputs.past_key_values,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","            cross_attentions=encoder_outputs.cross_attentions,\n","        )\n","    \n","    \n","# ------------------------------------------------- ROBERTA FOR TOKEN CLASSIFICATION -------------------------------------------------\n","\n","class CustomRobertaForTokenClassification(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = CustomRobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","        \n","\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        definition_inputs=None,      # New addition of definition_inputs\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            definition_inputs=definition_inputs,      # New addition of definition_inputs\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","    \n","# ----------------------------------------------------- MODEL SETUP -----------------------------------------------------\n","\n","class CustomRobertaCRF(nn.Module):\n","    '''\n","        Token BERT with (optional) Conditional Random Fileds (CRF)\n","    '''\n","\n","    def __init__(self, \n","                 num_labels,\n","                 model_name, \n","                 output_hidden_states=False,\n","                 output_attentions=False, \n","                 batch_first=True,\n","                 use_crf=True):\n","        \n","        super(CustomRobertaCRF, self).__init__()\n","        self.num_labels = num_labels\n","        self.batch_first = batch_first\n","        self.use_crf = use_crf\n","        self.base_model = CustomRobertaForTokenClassification.from_pretrained(\n","            model_name,\n","            num_labels=self.num_labels,\n","            output_hidden_states=output_hidden_states,\n","            output_attentions=output_attentions\n","        )\n","        if self.use_crf:\n","            self.crf = CRF(self.num_labels, batch_first=self.batch_first)\n","\n","            \n","            \n","            \n","    def forward(self,\n","                input_ids,\n","                attention_mask,\n","                definition_inputs,      # New addition of definition_inputs\n","                labels=None):\n","        \n","        outputs = self.base_model.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            definition_inputs=definition_inputs,      # New addition of definition_inputs\n","        )\n","        sequence_output = outputs[0]\n","        sequence_output = self.base_model.dropout(sequence_output) \n","        logits = self.base_model.classifier(sequence_output)\n","\n","        if self.use_crf:\n","            if labels is not None: # training\n","                loss = -self.crf(logits, labels, attention_mask.byte(), reduction='mean')\n","                preds = self.crf.decode(logits, attention_mask.byte())\n","                return loss, preds\n","            else: # inference\n","                return self.crf.decode(logits, attention_mask.byte())\n","        else:\n","            if labels is not None: # training\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(\n","                    logits.view(-1, self.num_labels),\n","                    labels.view(-1)\n","                )\n","                return loss\n","            else: # inference\n","                return torch.argmax(logits, dim=2)\n","            \n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:42:24.610451Z","iopub.status.busy":"2023-05-06T16:42:24.609790Z","iopub.status.idle":"2023-05-06T16:42:35.178559Z","shell.execute_reply":"2023-05-06T16:42:35.177455Z","shell.execute_reply.started":"2023-05-06T16:42:24.610420Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-crf\n","  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n","Installing collected packages: pytorch-crf\n","Successfully installed pytorch-crf-0.7.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install pytorch-crf"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:42:46.528938Z","iopub.status.busy":"2023-05-06T16:42:46.528256Z","iopub.status.idle":"2023-05-06T16:42:46.546094Z","shell.execute_reply":"2023-05-06T16:42:46.545170Z","shell.execute_reply.started":"2023-05-06T16:42:46.528906Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["            \n","# ----------------------------------------------------- DATA UTILS -----------------------------------------------------\n","           \n","def getData(tokenizer, token_data, span_start, span_end):\n","    \n","    mapList = []\n","    tokenizedList = []\n","    maxLength = 200\n","    for datapoint in token_data:\n","        \n","        curMap = {}\n","        curTokenList = [tokenizer.convert_tokens_to_ids(tokenizer.cls_token)]\n","        for i in range(len(datapoint)):\n","            curMap[i] = []\n","            curTk = tokenizer.tokenize(datapoint[i])  \n","            #iterate over subwords \n","            for tk in curTk:\n","                curMap[i].append(len(curTokenList))\n","                curTokenList.append(tokenizer.convert_tokens_to_ids(tk))\n","        curTokenList.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n","\n","        mapList.append(curMap)\n","        tokenizedList.append(curTokenList)\n","    \n","    #padding \n","    attn_mask = np.zeros((len(token_data), maxLength))\n","    for i in range(len(tokenizedList)):\n","        for j in range(len(tokenizedList[i])):\n","            attn_mask[i][j] = 1\n","        while (len(tokenizedList[i]) < maxLength):\n","            tokenizedList[i].append(tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n","\n","    #y value - [pad - 0, O - 1, I - 2, B - 3]\n","    y_val = np.ones((len(token_data), maxLength))\n","    for dp in range(len(span_start)):\n","        #overwrite points in ranges 'I'\n","        for i in range(len(span_start[dp])):\n","            for idx in range(span_start[dp][i], span_end[dp][i]+1):\n","                for k in mapList[dp][idx]:           \n","                    if (idx == span_start[dp][i]):\n","                        y_val[dp][k] = 3 #B\n","                    else:\n","                        y_val[dp][k] = 2 #I\n","        \n","    # mark the padded sequence as 'Not part of tweet': 3\n","    for i in range(len(token_data)):\n","        for j in range(maxLength):\n","            if (attn_mask[i][j] == 0):\n","                \n","                if (y_val[i][j] != 1):\n","                    print(\"Assertion_Failed\")\n","\n","                y_val[i][j] = 0\n","\n","    return np.array(tokenizedList), attn_mask, y_val, mapList\n","\n","\n","\n","\n","def getIOData(fileName: str,\n","              definition_path: str,\n","              tokenizer):\n","    data = pd.read_csv(fileName)\n","    token_data, span_start, span_end = [], [], []\n","    cnt_nonClaims = 0\n","    for i in range(len(data)):\n","        if (data['claim_label'][i] == 1):\n","            token_data.append(ast.literal_eval((data['tokens'][i])))\n","            span_start.append(json.loads(data['span_start_index'][i]))\n","            span_end.append(json.loads(data['span_end_index'][i]))\n","        elif (data['claim_label'][i] == 0):\n","            cnt_nonClaims += 1\n","    tokenizedList, attn_mask, y_val, mapList = getData(tokenizer, token_data, span_start, span_end)\n","    model_inputs = {}\n","    model_inputs['input_ids'] = torch.tensor([i for i in tokenizedList], dtype=torch.long, device=DEVICE)\n","    model_inputs['attention_mask'] = torch.tensor([i for i in attn_mask], dtype=torch.long, device=DEVICE)\n","    model_inputs['labels'] = torch.tensor([i for i in y_val], dtype=torch.long, device=DEVICE)\n","    model_inputs['definition_inputs'] = torch.unsqueeze(pd.read_pickle(definition_path), 0).repeat(model_inputs['input_ids'].shape[0], 1, 1, 1)\n","\n","    return model_inputs, mapList\n","\n","\n","\n","def set_up_data_loader(text_path: str,\n","                       definition_path: str,\n","                       tokenizer,\n","                       lowercase_utterances: bool=False,\n","                       unfolded_dialogue: bool=False):\n","    dataset, tweet_word_ids = getIOData(fileName=text_path, \n","                                        definition_path=definition_path,\n","                                        tokenizer=tokenizer)\n","    dataset = TensorDataset(dataset['input_ids'],\n","                            dataset['attention_mask'], \n","                            dataset['definition_inputs'],\n","                            dataset['labels'])\n","    return DataLoader(\n","        dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False\n","    ), tweet_word_ids    "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:42:48.871788Z","iopub.status.busy":"2023-05-06T16:42:48.871457Z","iopub.status.idle":"2023-05-06T16:42:48.892981Z","shell.execute_reply":"2023-05-06T16:42:48.892017Z","shell.execute_reply.started":"2023-05-06T16:42:48.871760Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# ----------------------------------------------------- METRIC UTILS -----------------------------------------------------\n","\n","def getPredictions(y_val, preds, mapList):\n","    \n","    groundTruth = []\n","    predictedIdx = []\n","    bioGround = []\n","    bioPreds = []\n","    #if any a subword lies in span - the word is in span\n","    for dp in range(len(y_val)):\n","        curG = []\n","        curP = []\n","        curBIO = []\n","        curBIOG = []\n","        for i in mapList[dp].keys():\n","            containsI_ground = False\n","            containsI_pred = False\n","            isBegin = False\n","            isBeginG = False\n","            for k in mapList[dp][i]:\n","                if (y_val[dp][k] >= 2):  #subword is I or B\n","                    containsI_ground = True\n","                    if (y_val[dp][k] == 3):\n","                        isBeginG = True\n","                if (preds[dp][k] >= 2):  #subword is I or B\n","                    containsI_pred = True\n","                    if (y_val[dp][k] == 3):\n","                        isBegin = True\n","                break           # We only consider first subword label in deciding the word level label\n","\n","            if (containsI_ground):\n","                curG.append(i)\n","                if (isBeginG):\n","                    curBIOG.append(1) #B\n","                else:\n","                    curBIOG.append(2) #I\n","            else:\n","                curBIOG.append(0) #O\n","            if (containsI_pred):\n","                if (isBegin):\n","                    curBIO.append(1) #B\n","                else:\n","                    curBIO.append(2) #I\n","                curP.append(i)\n","            else:\n","                curBIO.append(0) #O\n","\n","        groundTruth.append(curG)\n","        predictedIdx.append(curP)\n","        bioPreds.append(curBIO)\n","        bioGround.append(curBIOG)\n","    return groundTruth, predictedIdx, bioGround, bioPreds\n","\n","\n","\n","def get_token_list(labels):\n","    return [i for i in range(len(labels)) if labels[i]!=0]\n","\n","\n","def f1(predictions_bio, gold_bio):\n","    gold = get_token_list(gold_bio)\n","    predictions = get_token_list(predictions_bio)\n","    \n","    if len(gold) == 0:\n","        return 1 if len(predictions)==0 else 0\n","    nom = 2*len(set(predictions).intersection(set(gold)))\n","    denom = len(set(predictions))+len(set(gold))\n","    return nom/denom\n","\n","\n","def get_dice(ground, preds):\n","    tot = 0\n","    for i in range(len(preds)):\n","        tot += f1(ground[i], preds[i])\n","    return tot/len(preds)\n","\n","def get_hard_f1(ground, preds):\n","    return np.mean([f1_score(l, p, average='macro', zero_division=0) for l, p in list(zip(ground, preds))])\n","\n","\n","def get_hard_recall(ground, preds):\n","    return np.mean([recall_score(l, p, average='macro', zero_division=0) for l, p in list(zip(ground, preds))])\n","\n","\n","def get_hard_precision(ground, preds):\n","    return np.mean([precision_score(l, p, average='macro', zero_division=0) for l, p in list(zip(ground, preds))])\n","\n","\n","def get_hard_recall_classwise(ground, preds):\n","    return np.mean([recall_score(l, p, labels=[0,1,2], average=None, zero_division=0)  for l, p in list(zip(ground, preds))], axis=0)\n","\n","\n","def get_hard_precision_classwise(ground, preds):\n","    return np.mean([precision_score(l, p, labels=[0,1,2], average=None, zero_division=0) for l, p in list(zip(ground, preds))], axis=0)\n","\n","\n","def get_hard_f1_classwise(ground, preds):\n","    return np.mean([f1_score(l, p, labels=[0,1,2], average=None, zero_division=0) for l, p in list(zip(ground, preds))], axis=0)\n","\n","\n","def compute_metrics(ground, \n","                    preds):\n","    # f1 score for every tag\n","    hard_f1_classwise=[round(v, 4) for v in get_hard_f1_classwise(ground, preds)]\n","    f1_B = hard_f1_classwise[1]\n","    f1_I = hard_f1_classwise[2]\n","    f1_O = hard_f1_classwise[0]    \n","\n","    # precision\n","    hard_precision_classwise=[round(v, 4) for v in get_hard_precision_classwise(ground, preds)]\n","    precision_B = hard_precision_classwise[1]\n","    precision_I = hard_precision_classwise[2]\n","    precision_O = hard_precision_classwise[0] \n","\n","    # recall\n","    hard_recall_classwise=[round(v, 4) for v in get_hard_recall_classwise(ground, preds)]\n","    recall_B = hard_recall_classwise[1]\n","    recall_I = hard_recall_classwise[2]\n","    recall_O = hard_recall_classwise[0] \n","\n","    return {\n","        \"F1\": round(get_hard_f1(ground, preds), 4),\n","        \"Precision\": round(get_hard_precision(ground, preds), 4),\n","        \"Recall\": round(get_hard_recall(ground, preds), 4),\n","        \"F1_B\": round(f1_B, 4),\n","        \"F1_I\": round(f1_I, 4),\n","        \"F1_O\": round(f1_O, 4),\n","        \"Precision_B\": round(precision_B, 4),\n","        \"Precision_I\": round(precision_I, 4),\n","        \"Precision_O\": round(precision_O, 4),\n","        \"Recall_B\": round(recall_B, 4),\n","        \"Recall_I\": round(recall_I, 4),\n","        \"Recall_O\": round(recall_O, 4),\n","        \"DSC\": round(get_dice(ground, preds), 4)\n","    }"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:42:50.937124Z","iopub.status.busy":"2023-05-06T16:42:50.936639Z","iopub.status.idle":"2023-05-06T16:42:50.955667Z","shell.execute_reply":"2023-05-06T16:42:50.954719Z","shell.execute_reply.started":"2023-05-06T16:42:50.937096Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# ----------------------------------------------------- TRAINING UTILS -----------------------------------------------------\n","\n","def train_epoch(model,\n","                data_loader,\n","                optimizer):\n","    model.train()\n","    epoch_train_loss = 0.0\n","    for step, batch in enumerate(tqdm(data_loader, desc=\"Training Iteration\")):\n","        optimizer.zero_grad()\n","        \n","        with autocast():\n","            batch = tuple(t.to(DEVICE) for t in batch)\n","            input_ids, attention_mask, definition_inputs, labels = batch\n","\n","            outputs = model(input_ids=input_ids,\n","                            attention_mask=attention_mask,\n","                            definition_inputs=definition_inputs,\n","                            labels=labels)\n","            loss = outputs[0]\n","            epoch_train_loss += loss.item()\n","        \n","        SCALER.scale(loss).backward()\n","        SCALER.step(optimizer)\n","        SCALER.update()\n","    \n","    del batch\n","    del input_ids\n","    del attention_mask\n","    del definition_inputs\n","    del labels\n","    del outputs\n","    del loss\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","    return round(epoch_train_loss/ step, 4)\n","\n","\n","\n","\n","def val_epoch(model,\n","              tokenizer,\n","              data_loader):\n","    model.eval()\n","    \n","    epoch_val_loss = 0.0\n","    y_true = []\n","    y_pred = []\n","    \n","    for step, batch in enumerate(tqdm(data_loader, desc=\"Validation Loss Iteration\")):  \n","        \n","        with torch.no_grad():\n","            \n","            with autocast():\n","                \n","                batch = tuple(t.to(DEVICE) for t in batch)\n","                batch_input_ids, batch_attention_mask, definition_inputs, batch_labels = batch\n","                \n","                outputs = model(input_ids=batch_input_ids,\n","                                attention_mask=batch_attention_mask,\n","                                definition_inputs=definition_inputs,\n","                                labels=batch_labels)\n","                loss = outputs[0]\n","                epoch_val_loss += loss.item()  \n","                batch_tags = outputs[1]\n","                \n","            batch_labels = [labels.cpu().tolist() for labels in batch_labels]\n","            \n","            y_true.extend(batch_labels)\n","            y_pred.extend(batch_tags)\n","            \n","    del batch\n","    del batch_input_ids\n","    del batch_attention_mask\n","    del definition_inputs\n","    del batch_labels\n","    del batch_tags\n","    del outputs\n","    del loss\n","    gc.collect()\n","    torch.cuda.empty_cache() \n","        \n","    return round(epoch_val_loss/ step, 4), y_true, y_pred\n","\n","\n","\n","\n","def test_epoch(model,\n","               tokenizer,\n","               data_loader):\n","    model.eval()\n","    \n","    epoch_val_loss = 0.0\n","    y_true = []\n","    y_pred = []\n","    \n","    for step, batch in enumerate(tqdm(data_loader, desc=\"Inference Iteration\")):  \n","        \n","        with torch.no_grad():\n","            \n","            with autocast():\n","                \n","                batch = tuple(t.to(DEVICE) for t in batch)\n","                batch_input_ids, batch_attention_mask, definition_inputs, batch_labels = batch\n","                \n","                batch_tags = model(input_ids=batch_input_ids,\n","                                   attention_mask=batch_attention_mask, \n","                                   definition_inputs=definition_inputs)\n","                \n","            batch_labels = [labels.cpu().tolist() for labels in batch_labels]\n","            \n","            y_true.extend(batch_labels)\n","            y_pred.extend(batch_tags)\n","                \n","    del batch\n","    del batch_input_ids\n","    del batch_attention_mask\n","    del definition_inputs\n","    del batch_labels\n","    del batch_tags\n","    gc.collect()\n","    torch.cuda.empty_cache() \n","        \n","    return y_true, y_pred\n","\n","\n","\n","\n","def prepare_for_training(model, \n","                         base_learning_rate: float):\n","    optimizer=AdamW(model.parameters(), \n","                    lr=base_learning_rate)   \n","    gc.collect()\n","    torch.cuda.empty_cache() \n","    return optimizer\n","\n","\n","\n","\n","def train(model,\n","          tokenizer,\n","          train_data_loader,\n","          base_learning_rate,\n","          data_word_ids):\n","    \n","    optimizer = prepare_for_training(model=model,\n","                                     base_learning_rate=base_learning_rate)\n","    \n","    train_losses = []\n","    val_losses = []\n","    \n","    for epoch in range(MAX_EPOCHS):\n","        train_loss = train_epoch(model,\n","                                 train_data_loader, \n","                                 optimizer)\n","        train_losses.append(train_loss)\n","        \n","        # Compute Metrics\n","        train_preds, train_preds = test_epoch(model,\n","                                              tokenizer,\n","                                              train_data_loader)\n","        \n","        print(\"\\nEpoch: {}\\ttrain_loss: {}\".format(epoch+1, train_loss))\n","        \n","        train_label_idx, train_preds_idx, train_label_bio, train_preds_bio = getPredictions(train_preds,\n","                                                                                             train_preds, \n","                                                                                             data_word_ids['train_word_ids'])\n","        train_results = compute_metrics(train_label_bio, train_preds_bio)\n","        print(\"Train Results: \", train_results)\n","               \n","        torch.save(model,'/kaggle/working/'  +\"DABERTa_epoch_\" + str(epoch+1) + \"_\" + datetime.now().strftime('%d-%m-%Y-%H:%M') + \".pt\")\n","        \n","        del train_loss\n","        gc.collect()\n","        torch.cuda.empty_cache() \n","      "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T16:43:02.368819Z","iopub.status.busy":"2023-05-06T16:43:02.368453Z","iopub.status.idle":"2023-05-06T16:43:02.375612Z","shell.execute_reply":"2023-05-06T16:43:02.374706Z","shell.execute_reply.started":"2023-05-06T16:43:02.368777Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["def main():\n","    MODEL = CustomRobertaCRF(num_labels=4,\n","                             model_name='roberta-base', \n","                             output_hidden_states=False,\n","                             output_attentions=False, \n","                             batch_first=True,\n","                             use_crf=True)\n","    print(\"Model loaded...\\n\")\n","    MODEL.to(DEVICE)\n","\n","    TOKENIZER = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n","    print(\"Tokenizer loaded...\\n\")\n","\n","    \n","    # ------------------------------ READ DATASET ------------------------------ #\n","    \n","    train_dataset, train_word_ids = set_up_data_loader(text_path='/kaggle/input/meme-span-datatset/train.csv', \n","                                                       definition_path='/kaggle/input/meme-span-datatset/claim_desc_new_vecs_base.pkl', \n","                                                       tokenizer=TOKENIZER)\n","    print(\"\\nTraining Data Loaded...\")\n","\n","    gc.collect()  \n","    \n","    # ------------------------------ TRAINING SETUP ------------------------------ #\n","        \n","    data_word_ids = {\n","        'train_word_ids': train_word_ids\n","    }\n","    \n","    train(model=MODEL,\n","          tokenizer=TOKENIZER,\n","          train_data_loader=train_dataset,\n","          base_learning_rate=BASE_LEARNING_RATE,\n","          data_word_ids=data_word_ids)\n","    \n","    print(\"Model Trained!\")\n","    \n","    gc.collect()\n","    torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-05-06T16:43:07.243453Z","iopub.status.busy":"2023-05-06T16:43:07.243084Z","iopub.status.idle":"2023-05-06T17:20:15.697247Z","shell.execute_reply":"2023-05-06T17:20:15.696238Z","shell.execute_reply.started":"2023-05-06T16:43:07.243423Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5faccc3af90e4fc998cbe47c58f2b53d","version_major":2,"version_minor":0},"text/plain":["Downloading ()lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0ac7461900d4438bfb7e88f58227ea8","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing CustomRobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing CustomRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing CustomRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of CustomRobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.desc_net.attention_layer.fc.bias', 'roberta.encoder.desc_net.igm_layer.w_c4.weight', 'roberta.encoder.desc_net.igm_layer.b_mu_r', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.linear2.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.norm2.weight', 'roberta.encoder.desc_net.fc.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.norm1.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.norm1.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.norm2.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.linear2.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.norm1.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.self_attn.in_proj_bias', 'roberta.encoder.desc_net.def_encoder.pos_encoder.pe', 'classifier.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.norm2.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.self_attn.out_proj.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.norm1.bias', 'roberta.encoder.desc_net.attention_layer.fc.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.linear2.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.linear1.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.linear1.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.self_attn.in_proj_bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.linear1.bias', 'roberta.encoder.desc_net.attention_layer.key_transform.weight', 'roberta.encoder.desc_net.attention_layer.value_transform.weight', 'roberta.encoder.desc_net.igm_layer.b_a', 'roberta.encoder.desc_net.igm_layer.w_c2.weight', 'roberta.encoder.desc_net.fc.weight', 'roberta.encoder.desc_net.attention_layer.query_transform.weight', 'roberta.encoder.desc_net.igm_layer.b_r', 'roberta.encoder.desc_net.igm_layer.b_c', 'roberta.encoder.desc_net.igm_layer.w_c1.weight', 'roberta.encoder.desc_net.igm_layer.w_r2.weight', 'roberta.encoder.desc_net.attention_layer.layer_norm.weight', 'roberta.encoder.desc_net.igm_layer.b_mu_c', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.self_attn.out_proj.weight', 'roberta.encoder.desc_net.attention_layer.layer_norm.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.self_attn.in_proj_weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.linear1.weight', 'roberta.encoder.desc_net.igm_layer.w_r4.weight', 'roberta.encoder.desc_net.igm_layer.w_a.weight', 'classifier.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.norm2.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.self_attn.out_proj.bias', 'roberta.encoder.desc_net.igm_layer.w_c3.weight', 'roberta.encoder.desc_net.igm_layer.w_r3.weight', 'roberta.encoder.desc_net.igm_layer.w_r1.weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.0.self_attn.in_proj_weight', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.linear2.bias', 'roberta.encoder.desc_net.def_encoder.encoder.layers.1.self_attn.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded...\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cd792ff47804702a0592913bfc2ab1f","version_major":2,"version_minor":0},"text/plain":["Downloading ()olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b3452fa420f43ba8a2906064fe5754f","version_major":2,"version_minor":0},"text/plain":["Downloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da70c6fae68a45af840a33691730b996","version_major":2,"version_minor":0},"text/plain":["Downloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizer loaded...\n","\n","\n","Training Data Loaded...\n"]},{"name":"stderr","output_type":"stream","text":["Training Iteration: 100%|| 378/378 [05:06<00:00,  1.23it/s]\n","Inference Iteration: 100%|| 378/378 [01:31<00:00,  4.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 1\ttrain_loss: 22.0732\n","Train Results:  {'F1': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1_B': 0.9199, 'F1_I': 0.9975, 'F1_O': 0.9343, 'Precision_B': 0.9199, 'Precision_I': 0.9975, 'Precision_O': 0.9343, 'Recall_B': 0.9199, 'Recall_I': 0.9975, 'Recall_O': 0.9343, 'DSC': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Training Iteration: 100%|| 378/378 [05:04<00:00,  1.24it/s]\n","Inference Iteration: 100%|| 378/378 [01:31<00:00,  4.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 2\ttrain_loss: 14.1603\n","Train Results:  {'F1': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1_B': 0.9639, 'F1_I': 0.9992, 'F1_O': 0.9265, 'Precision_B': 0.9639, 'Precision_I': 0.9992, 'Precision_O': 0.9265, 'Recall_B': 0.9639, 'Recall_I': 0.9992, 'Recall_O': 0.9265, 'DSC': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Training Iteration: 100%|| 378/378 [05:03<00:00,  1.25it/s]\n","Inference Iteration: 100%|| 378/378 [01:30<00:00,  4.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 3\ttrain_loss: 10.5551\n","Train Results:  {'F1': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1_B': 0.9838, 'F1_I': 0.9987, 'F1_O': 0.9186, 'Precision_B': 0.9838, 'Precision_I': 0.9987, 'Precision_O': 0.9186, 'Recall_B': 0.9838, 'Recall_I': 0.9987, 'Recall_O': 0.9186, 'DSC': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Training Iteration: 100%|| 378/378 [05:02<00:00,  1.25it/s]\n","Inference Iteration: 100%|| 378/378 [01:31<00:00,  4.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 4\ttrain_loss: 7.8149\n","Train Results:  {'F1': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1_B': 0.9909, 'F1_I': 0.9987, 'F1_O': 0.9224, 'Precision_B': 0.9909, 'Precision_I': 0.9987, 'Precision_O': 0.9224, 'Recall_B': 0.9909, 'Recall_I': 0.9987, 'Recall_O': 0.9224, 'DSC': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":["Training Iteration: 100%|| 378/378 [05:08<00:00,  1.23it/s]\n","Inference Iteration: 100%|| 378/378 [01:30<00:00,  4.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch: 5\ttrain_loss: 6.3068\n","Train Results:  {'F1': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1_B': 0.9833, 'F1_I': 0.9964, 'F1_O': 0.9244, 'Precision_B': 0.9833, 'Precision_I': 0.9964, 'Precision_O': 0.9244, 'Recall_B': 0.9833, 'Recall_I': 0.9964, 'Recall_O': 0.9244, 'DSC': 1.0}\n","Model Trained!\n"]}],"source":["torch.cuda.empty_cache() \n","main()"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T17:25:29.584684Z","iopub.status.busy":"2023-05-06T17:25:29.584000Z","iopub.status.idle":"2023-05-06T17:25:29.595049Z","shell.execute_reply":"2023-05-06T17:25:29.592856Z","shell.execute_reply.started":"2023-05-06T17:25:29.584651Z"},"trusted":true},"outputs":[],"source":["d1 = { 'F1_B': 0.9199, 'F1_I': 0.9975, 'F1_O': 0.9343, 'Precision_B': 0.9199, 'Precision_I': 0.9975, 'Precision_O': 0.9343, 'Recall_B': 0.9199, 'Recall_I': 0.9975, 'Recall_O': 0.9343, 'DSC': 1.0}\n","d2 = { 'F1_B': 0.9639, 'F1_I': 0.9992, 'F1_O': 0.9265, 'Precision_B': 0.9639, 'Precision_I': 0.9992, 'Precision_O': 0.9265, 'Recall_B': 0.9639, 'Recall_I': 0.9992, 'Recall_O': 0.9265, 'DSC': 1.0}\n","d3 = { 'F1_B': 0.9838, 'F1_I': 0.9987, 'F1_O': 0.9186, 'Precision_B': 0.9838, 'Precision_I': 0.9987, 'Precision_O': 0.9186, 'Recall_B': 0.9838, 'Recall_I': 0.9987, 'Recall_O': 0.9186, 'DSC': 1.0}\n","d4 = { 'F1_B': 0.9909, 'F1_I': 0.9987, 'F1_O': 0.9224, 'Precision_B': 0.9909, 'Precision_I': 0.9987, 'Precision_O': 0.9224, 'Recall_B': 0.9909, 'Recall_I': 0.9987, 'Recall_O': 0.9224, 'DSC': 1.0}\n","d5 = { 'F1_B': 0.9833, 'F1_I': 0.9964, 'F1_O': 0.9244, 'Precision_B': 0.9833, 'Precision_I': 0.9964, 'Precision_O': 0.9244, 'Recall_B': 0.9833, 'Recall_I': 0.9964, 'Recall_O': 0.9244, 'DSC': 1.0}\n","dl = [d1,d2,d3,d4,d5]\n","keys = list(d1.keys())\n","df = {key:[] for key in keys}\n","for i in range(5):\n","    for k in keys:\n","        df[k].append(dl[i][k])\n","        \n","df = pd.DataFrame(df)\n","\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-05-06T17:26:41.664578Z","iopub.status.busy":"2023-05-06T17:26:41.663904Z","iopub.status.idle":"2023-05-06T17:26:41.938155Z","shell.execute_reply":"2023-05-06T17:26:41.936963Z","shell.execute_reply.started":"2023-05-06T17:26:41.664545Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<AxesSubplot: >"]},"execution_count":17,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["df['F1_B'].plot()\n","df['F1_I'].plot()\n","df['F1_O'].plot()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":4}
