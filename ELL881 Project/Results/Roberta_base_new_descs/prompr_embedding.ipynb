{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from transformers import RobertaForTokenClassification, RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "class prompt(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 wte : nn.Embedding,\n",
    "                 prompt_length : int = 20,\n",
    "                 rand_range : float = 0.5,\n",
    "                 initialize_from_vocab : bool = True):\n",
    "\n",
    "        super(prompt, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.prompt_length = prompt_length\n",
    "        self.learned_embedding = nn.Parameter(self.initialize_embedding(wte,\n",
    "                                                                        prompt_length,\n",
    "                                                                        rand_range,\n",
    "                                                                        initialize_from_vocab))\n",
    "\n",
    "    \n",
    "    def initialize_embedding(self,\n",
    "                             wte : nn.Embedding,\n",
    "                             prompt_length : int =10,\n",
    "                             random_range : float = 0.5,\n",
    "                             initialize_from_vocab : bool = True):\n",
    "\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:prompt_length].clone().detach()\n",
    "\n",
    "        return torch.FloatTensor(wte.weight.size(1),prompt_length).uniform_(-random_range,random_range)\n",
    "\n",
    "    def forward(self,tokens):\n",
    "        input_embedding = self.wte(tokens[:,self.prompt_length:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0),1,1)\n",
    "        print(input_embedding)\n",
    "        print(input_embedding.shape)\n",
    "        print(learned_embedding)\n",
    "        print(learned_embedding.shape)\n",
    "        return torch.cat((learned_embedding,input_embedding),dim=1)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels = 1,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False, \n",
    ")\n",
    "prompt_emb = prompt(model.get_input_embeddings(), \n",
    "                      prompt_length=20, \n",
    "                      initialize_from_vocab=True)\n",
    "model.set_input_embeddings(prompt_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Hello, my dog is cute\")\n",
    "\n",
    "n_tokens = 20\n",
    "inputs['input_ids']=torch.cat((torch.full((1,n_tokens), 500).resize(n_tokens),torch.tensor(inputs['input_ids'], dtype=torch.long)))\n",
    "inputs['attention_mask']=torch.cat((torch.full((1,n_tokens), 1).resize(n_tokens),torch.tensor(inputs['attention_mask'], dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "torch.Size([28])\n",
      "tensor([[[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
      "         [-0.1852,  0.0297, -0.1843,  ...,  0.0050, -0.1139,  0.0404],\n",
      "         [ 0.0998, -0.0595, -0.0705,  ..., -0.2491, -0.0905, -0.0499],\n",
      "         ...,\n",
      "         [-0.0265,  0.0249,  0.0118,  ...,  0.0011, -0.0384, -0.1366],\n",
      "         [-0.0861,  0.2069,  0.0929,  ...,  0.1931, -0.1449, -0.0831],\n",
      "         [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 8, 768])\n",
      "tensor([[[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
      "         [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
      "         [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
      "         ...,\n",
      "         [-0.1332, -0.0391, -0.0661,  ..., -0.0450, -0.0546,  0.0156],\n",
      "         [ 0.0358,  0.0647, -0.1526,  ..., -0.1164, -0.0242, -0.0792],\n",
      "         [-0.0911, -0.1117, -0.0304,  ..., -0.0569, -0.1113, -0.1200]]],\n",
      "       grad_fn=<RepeatBackward0>)\n",
      "torch.Size([1, 20, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[ 0.2737],\n",
       "         [ 0.1345],\n",
       "         [ 0.1662],\n",
       "         [-0.0013],\n",
       "         [ 0.0019],\n",
       "         [-0.0683],\n",
       "         [-0.0614],\n",
       "         [-0.0914],\n",
       "         [-0.1765],\n",
       "         [-0.1113],\n",
       "         [-0.0162],\n",
       "         [-0.1161],\n",
       "         [-0.0499],\n",
       "         [-0.0083],\n",
       "         [-0.1131],\n",
       "         [ 0.1061],\n",
       "         [ 0.1515],\n",
       "         [ 0.1186],\n",
       "         [ 0.1165],\n",
       "         [-0.0790],\n",
       "         [ 0.2720],\n",
       "         [ 0.0201],\n",
       "         [ 0.1880],\n",
       "         [ 0.0847],\n",
       "         [ 0.1159],\n",
       "         [ 0.1400],\n",
       "         [-0.0990],\n",
       "         [ 0.2838]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_in = torch.LongTensor(inputs['input_ids'])\n",
    "batch_mask = torch.LongTensor(inputs['attention_mask'])\n",
    "print(batch_in.shape)\n",
    "print(batch_mask.shape)\n",
    "batch_in = batch_in.reshape((1,batch_in.shape[0]))\n",
    "batch_mask = batch_mask.reshape((1,batch_mask.shape[0]))    \n",
    "model(batch_in,batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(\n",
    "    \"Creampie motherfucker I would like to fuck you\")\n",
    "\n",
    "n_tokens = 20\n",
    "inputs['input_ids']=torch.cat((torch.full((1,n_tokens), 500).resize(n_tokens),torch.tensor(inputs['input_ids'], dtype=torch.long)))\n",
    "inputs['attention_mask']=torch.cat((torch.full((1,n_tokens), 1).resize(n_tokens),torch.tensor(inputs['attention_mask'], dtype=torch.long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
      "         [-0.3821, -0.0012,  0.0288,  ..., -0.0869, -0.1298, -0.0964],\n",
      "         [ 0.0670,  0.1276, -0.1354,  ..., -0.0532,  0.1196,  0.1250],\n",
      "         ...,\n",
      "         [ 0.0407, -0.3547, -0.0796,  ..., -0.2576,  0.0377, -0.2172],\n",
      "         [ 0.0184, -0.0451, -0.0091,  ...,  0.1189, -0.0917, -0.0632],\n",
      "         [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([1, 14, 768])\n",
      "tensor([[[ 0.1476, -0.0365,  0.0753,  ..., -0.0023,  0.0172, -0.0016],\n",
      "         [ 0.0156,  0.0076, -0.0118,  ..., -0.0022,  0.0081, -0.0156],\n",
      "         [-0.0347, -0.0873, -0.0180,  ...,  0.1174, -0.0098, -0.0355],\n",
      "         ...,\n",
      "         [-0.1332, -0.0391, -0.0661,  ..., -0.0450, -0.0546,  0.0156],\n",
      "         [ 0.0358,  0.0647, -0.1526,  ..., -0.1164, -0.0242, -0.0792],\n",
      "         [-0.0911, -0.1117, -0.0304,  ..., -0.0569, -0.1113, -0.1200]]],\n",
      "       grad_fn=<RepeatBackward0>)\n",
      "torch.Size([1, 20, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[-0.2895],\n",
       "         [-0.2569],\n",
       "         [-0.2413],\n",
       "         [-0.2783],\n",
       "         [-0.1996],\n",
       "         [-0.1514],\n",
       "         [-0.1752],\n",
       "         [-0.0555],\n",
       "         [-0.0038],\n",
       "         [ 0.0520],\n",
       "         [-0.1599],\n",
       "         [-0.0871],\n",
       "         [-0.1321],\n",
       "         [-0.1538],\n",
       "         [-0.1403],\n",
       "         [-0.0408],\n",
       "         [-0.0910],\n",
       "         [-0.1631],\n",
       "         [-0.1798],\n",
       "         [-0.0919],\n",
       "         [-0.2899],\n",
       "         [-0.4656],\n",
       "         [-0.0797],\n",
       "         [-0.2300],\n",
       "         [-0.4179],\n",
       "         [-0.2770],\n",
       "         [-0.1964],\n",
       "         [-0.1745],\n",
       "         [-0.2441],\n",
       "         [-0.2134],\n",
       "         [-0.3117],\n",
       "         [-0.2743],\n",
       "         [-0.4425],\n",
       "         [-0.2813]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_in = torch.LongTensor(inputs['input_ids'])\n",
    "batch_mask = torch.LongTensor(inputs['attention_mask'])\n",
    "batch_in = batch_in.reshape((1,batch_in.shape[0]))\n",
    "batch_mask = batch_mask.reshape((1,batch_mask.shape[0]))    \n",
    "model(batch_in,batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenizer('Hellow sanya. Lets do it',return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   725, 33796,   579, 12837,     4, 40702,   109,    24,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok['input_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   725, 33796,   579, 12837,     4, 40702,   109,    24,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [42891, 1669, 939, 524, 45, 7359, 571, 3320, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:761: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "toks = tokenizer(\"hello guys i am not chatgpt\",add_special_tokens =False,padding = 'max_length',max_length=50)\n",
    "print(toks)\n",
    "toks['input_ids'] = torch.cat((torch.full((1,n_tokens), 500).resize(n_tokens),torch.tensor(toks['input_ids'], dtype=torch.long)))\n",
    "toks['input_ids'] = toks['input_ids'].reshape((1,toks['input_ids'].shape[0]))\n",
    "            \n",
    "toks['attention_mask'] = torch.cat((torch.full((1,n_tokens), 1).resize(n_tokens),torch.tensor(toks['attention_mask'], dtype=torch.long)))\n",
    "toks['attention_mask'] = toks['attention_mask'].reshape((1,toks['attention_mask'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  500,   500,   500,   500,   500,   500,   500,   500,   500,   500,\n",
       "           500,   500,   500,   500,   500,   500,   500,   500,   500,   500,\n",
       "         42891,  1669,   939,   524,    45,  7359,   571,  3320,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[42891,  1669,   939,   524,    45,  7359,   571,  3320,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello guys i am not chatgpt\",add_special_tokens =False,padding = 'max_length',max_length=50,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
