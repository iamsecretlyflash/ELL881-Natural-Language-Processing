{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.optim import SGD,AdamW\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForTokenClassification\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(df,add_sep_prompt=True):\n",
    "    maxl = 0\n",
    "    labels = []\n",
    "    texts = []\n",
    "    label_map = {'<pad>':3, 'O':0, 'I':1, 'B':2}\n",
    "    attention_masks = []\n",
    "    for i in range(len(df)):\n",
    "        text = ast.literal_eval(df['tokens'][i])\n",
    "        texts.append(text)\n",
    "        spans = [(i,j) for i,j in zip(ast.literal_eval(df['span_start_index'][i]), ast.literal_eval(df['span_end_index'][i]))]\n",
    "        label = ['O']*len(text)\n",
    "        for span in spans:\n",
    "            label[span[0]] = 'B'\n",
    "            for i in range(span[0]+1, span[1]+1):\n",
    "                label[i] = 'I'\n",
    "        labels.append(label)\n",
    "\n",
    "    tokens = []\n",
    "    labels_final = []\n",
    "    map_list = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        text = texts[i]\n",
    "        tokens_list = [500]*PROMPT_LENGTH\n",
    "        labels_list = [label_map['O']]*PROMPT_LENGTH\n",
    "        cur_map = {}\n",
    "\n",
    "        #adding seperation token\n",
    "        if add_sep_prompt:\n",
    "            tokens_list.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "            labels_list.append(label_map['O'])\n",
    "\n",
    "        for j in range(len(text)):\n",
    "            tokenized = tokenizer.tokenize(text[j])\n",
    "            cur_map[j] = []\n",
    "            for sub_token in tokenized:\n",
    "                cur_map[j].append(len(tokens_list))\n",
    "                tokens_list.append(tokenizer.convert_tokens_to_ids(sub_token))\n",
    "        \n",
    "        for k in range(len(text)):\n",
    "            for j in cur_map[k]:\n",
    "                labels_list.append(label_map[labels[i][k]])\n",
    "        \n",
    "        #Adding seperation token\n",
    "        tokens_list.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "        labels_list.append(label_map['O'])\n",
    "        attention_mask = [1]*len(tokens_list)\n",
    "\n",
    "        tokens_list.extend([(tokenizer.pad_token_id)]*(MAX_LEN - len(tokens_list)))\n",
    "        labels_list.extend([label_map['O']]*(MAX_LEN - len(labels_list)))\n",
    "        attention_mask.extend([0]*(MAX_LEN - len(attention_mask)))\n",
    "\n",
    "        labels_final.append(labels_list)\n",
    "        tokens.append(tokens_list)\n",
    "        attention_masks.append(attention_mask)\n",
    "        map_list.append(cur_map)\n",
    "\n",
    "    return tokens, labels_final, attention_masks, map_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataForBert(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        super(DataForBert, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.tokens, self.labels, self.attention_masks, self.map_list = tokenize_and_align(df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.LongTensor(self.tokens[idx]), torch.LongTensor(self.labels[idx]), torch.LongTensor(self.attention_masks[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prompt(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 wte : nn.Embedding,\n",
    "                 prompt_length : int = 20,\n",
    "                 rand_range : float = 0.5,\n",
    "                 initialize_from_vocab : bool = True):\n",
    "\n",
    "        super(prompt, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.prompt_length = prompt_length\n",
    "        self.learned_embedding = nn.Parameter(self.initialize_embedding(wte,\n",
    "                                                                        prompt_length,\n",
    "                                                                        rand_range,\n",
    "                                                                        initialize_from_vocab))\n",
    "\n",
    "    \n",
    "    def initialize_embedding(self,\n",
    "                             wte : nn.Embedding,\n",
    "                             prompt_length : int =10,\n",
    "                             random_range : float = 0.5,\n",
    "                             initialize_from_vocab : bool = True):\n",
    "\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:prompt_length].clone().detach()\n",
    "\n",
    "        return torch.FloatTensor(wte.weight.size(1),prompt_length).uniform_(-random_range,random_range)\n",
    "\n",
    "    def forward(self,tokens):\n",
    "        input_embedding = self.wte(tokens[:,self.prompt_length:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0),1,1)\n",
    "        return torch.cat((learned_embedding,input_embedding),dim=1)\n",
    "                    \n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 prompt_length = 20,\n",
    "                 initialize_from_vocab = True):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=3)\n",
    "        if prompt_length>0:\n",
    "            self.prompt_length = prompt_length\n",
    "            self.prompt = prompt(self.bert.get_input_embeddings(),\n",
    "                                prompt_length = self.prompt_length,\n",
    "                                initialize_from_vocab = initialize_from_vocab)\n",
    "            \n",
    "            self.bert.set_input_embeddings(self.prompt)\n",
    "\n",
    "    def forward(self, input_id, label, mask):\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model,df_train, df_val,EPOCHS,LEARNING_RATE,BATCH_SIZE,optim = \"SGD\",):\n",
    "\n",
    "    train_data = DataForBert(df_train)\n",
    "    val_data = DataForBert(df_val)\n",
    "\n",
    "    train_loader = DataLoader(train_data,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_data,  batch_size=BATCH_SIZE)\n",
    "\n",
    "    if optim == \"AdamW\":\n",
    "        optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    elif optim == \"SGD\":\n",
    "        optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if DEVICE == 'cuda':\n",
    "        model = model.cuda()\n",
    "\n",
    "    epoch_data = []\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        \n",
    "        total_acc_train_o = 0\n",
    "        print(model.prompt.learned_embedding)\n",
    "        model.train()\n",
    "\n",
    "        for train_in, train_out,train_mask in tqdm(train_loader):\n",
    "\n",
    "            train_out = train_out.to(DEVICE)\n",
    "            train_in = train_in.to(DEVICE)\n",
    "            mask = train_mask.to(DEVICE)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(train_in, train_out, mask)\n",
    "            \n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean= logits[i][train_out[i] != -100]\n",
    "              label_clean = train_out[i][train_out[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "                \n",
    "              logits_clean2 = logits[i][train_out[i] != -100]\n",
    "              label_clean2 = train_out[i][train_out[i] != -100]\n",
    "              logits_clean2 = logits_clean2[label_clean2 != 1]\n",
    "              label_clean2 = label_clean2[label_clean2 != 1]\n",
    "\n",
    "              predictions2 = logits_clean2.argmax(dim=1)\n",
    "              acc2 = (predictions2 == label_clean2).float().mean()\n",
    "                \n",
    "              if len(predictions2)>0 and len(label_clean2)>0:\n",
    "                total_acc_train_o += acc2\n",
    "              \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_acc_val_o = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_in, val_out, val_mask in val_loader:\n",
    "            \n",
    "\n",
    "            val_out = val_out.to(DEVICE)\n",
    "            val_in = val_in.to(DEVICE)\n",
    "            mask = val_mask.to(DEVICE)\n",
    "\n",
    "            loss, logits = model(val_in, val_out, mask)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][val_out[i] != -100]\n",
    "              label_clean = val_out[i][val_out[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_val += acc\n",
    "              total_loss_val += loss.item()\n",
    "                \n",
    "              logits_clean2 = logits[i][val_out[i] != -100]\n",
    "              label_clean2 = val_out[i][val_out[i] != -100]\n",
    "              logits_clean2 = logits_clean2[label_clean2 != 1]\n",
    "              label_clean2 = label_clean2[label_clean2 != 1]\n",
    "              \n",
    "              \n",
    "              predictions2 = logits_clean2.argmax(dim=1)\n",
    "              acc2 = (predictions2 == label_clean2).float().mean()\n",
    "              if len(predictions2)>0 and len(label_clean2)>0:\n",
    "                total_acc_val_o += acc2\n",
    "                \n",
    "        val_accuracy = (total_acc_val / len(df_val))\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "        print(f'Accuracy train w/o O {total_acc_train_o / len(df_train): .3f} | Accuracy val w/o O = {total_acc_val_o / len(df_val): .3f}')\n",
    "        \n",
    "        epoch_data.append((total_loss_train / len(df_train),total_acc_train / len(df_train),total_acc_train_o / len(df_train),total_loss_val / len(df_val),val_accuracy,total_acc_val_o / len(df_val)))\n",
    "    return epoch_data,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(maxl=256,batch_size=2,epochs=5,learning_rate=1e-5,optim = \"SGD\"):\n",
    "    \n",
    "    df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9 * len(df))])\n",
    "    \n",
    "    model = BertModel()\n",
    "    ep_data,model = train_loop(model,df_train,df_val,epochs,learning_rate,batch_size,optim= optim)\n",
    "    \n",
    "    train_loss,train_acc,train_acc_o,val_loss,val_acc,val_acc_o = [],[],[],[],[],[]\n",
    "    \n",
    "    for a,b,c,d,e,f in ep_data:\n",
    "        try:\n",
    "            train_loss.append(a.item())\n",
    "        except:\n",
    "            train_loss.append(a)\n",
    "        try:\n",
    "            train_acc.append(b.item())\n",
    "        except:\n",
    "            train_acc.append(b)\n",
    "        try:\n",
    "            train_acc_o.append(c.item())\n",
    "        except:\n",
    "            train_acc_o.append(c)\n",
    "        try:\n",
    "            val_loss.append(d.item())\n",
    "        except:\n",
    "            val_loss.append(d)\n",
    "        try:\n",
    "            val_acc.append(e.item())\n",
    "        except:\n",
    "            val_acc.append(e)\n",
    "        try:\n",
    "            val_acc_o.append(f.item())\n",
    "        except:\n",
    "            val_acc_o.append(f)\n",
    "        \n",
    "    print(train_loss,train_acc,train_acc_o,val_loss,val_acc,val_acc_o)\n",
    "    loss_metrics = pd.DataFrame({'Train loss':train_loss,\n",
    "               'Val loss':val_loss\n",
    "               })\n",
    "    \n",
    "    acc_metrics = pd.DataFrame({'Train Accuracy':train_acc,\n",
    "                                'Train Accuracy without O':train_acc_o,\n",
    "                                'Val Accuracy':val_acc,\n",
    "                                'Val Accuracy without O':val_acc_o})\n",
    "    \n",
    "    loss_metrics.plot()\n",
    "    acc_metrics.plot()\n",
    "    return df_test, model, (loss_metrics,acc_metrics)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/anamikaseth/Desktop/ELL881 Project/bio_prompting_project.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=0'>1</a>\u001b[0m test_pipeline()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;32m/Users/anamikaseth/Desktop/ELL881 Project/bio_prompting_project.ipynb Cell 9\u001b[0m in \u001b[0;36mtest_pipeline\u001b[0;34m(maxl, batch_size, epochs, learning_rate, optim)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=2'>3</a>\u001b[0m df_train, df_val, df_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(df\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=3'>4</a>\u001b[0m                         [\u001b[39mint\u001b[39m(\u001b[39m.8\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df)), \u001b[39mint\u001b[39m(\u001b[39m.9\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df))])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m BertModel()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=6'>7</a>\u001b[0m ep_data,model \u001b[39m=\u001b[39m train_loop(model,df_train,df_val,epochs,learning_rate,batch_size,optim\u001b[39m=\u001b[39;49m optim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=8'>9</a>\u001b[0m train_loss,train_acc,train_acc_o,val_loss,val_acc,val_acc_o \u001b[39m=\u001b[39m [],[],[],[],[],[]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m a,b,c,d,e,f \u001b[39min\u001b[39;00m ep_data:\n",
      "\u001b[1;32m/Users/anamikaseth/Desktop/ELL881 Project/bio_prompting_project.ipynb Cell 9\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, df_train, df_val, EPOCHS, LEARNING_RATE, BATCH_SIZE, optim)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_loop\u001b[39m(model,df_train, df_val,EPOCHS,LEARNING_RATE,BATCH_SIZE,optim \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSGD\u001b[39m\u001b[39m\"\u001b[39m,):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=2'>3</a>\u001b[0m     train_data \u001b[39m=\u001b[39m DataForBert(df_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=3'>4</a>\u001b[0m     val_data \u001b[39m=\u001b[39m DataForBert(df_val)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=5'>6</a>\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(train_data,  batch_size\u001b[39m=\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m/Users/anamikaseth/Desktop/ELL881 Project/bio_prompting_project.ipynb Cell 9\u001b[0m in \u001b[0;36mDataForBert.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=3'>4</a>\u001b[0m \u001b[39msuper\u001b[39m(DataForBert, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m df\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_masks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap_list \u001b[39m=\u001b[39m tokenize_and_align(df)\n",
      "\u001b[1;32m/Users/anamikaseth/Desktop/ELL881 Project/bio_prompting_project.ipynb Cell 9\u001b[0m in \u001b[0;36mtokenize_and_align\u001b[0;34m(df, add_sep_prompt)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=5'>6</a>\u001b[0m attention_masks \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=7'>8</a>\u001b[0m     text \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mliteral_eval(df[\u001b[39m'\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m'\u001b[39;49m][i])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=8'>9</a>\u001b[0m     texts\u001b[39m.\u001b[39mappend(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anamikaseth/Desktop/ELL881%20Project/bio_prompting_project.ipynb#ch0000021?line=9'>10</a>\u001b[0m     spans \u001b[39m=\u001b[39m [(i,j) \u001b[39mfor\u001b[39;00m i,j \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(ast\u001b[39m.\u001b[39mliteral_eval(df[\u001b[39m'\u001b[39m\u001b[39mspan_start_index\u001b[39m\u001b[39m'\u001b[39m][i]), ast\u001b[39m.\u001b[39mliteral_eval(df[\u001b[39m'\u001b[39m\u001b[39mspan_end_index\u001b[39m\u001b[39m'\u001b[39m][i]))]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:982\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    981\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 982\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    985\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:1092\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "#Defining constants\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "PROMPT_LENGTH = 20\n",
    "f_test2k_np, model2k_np, metrics2k_np = test_pipeline()\n",
    "print()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
