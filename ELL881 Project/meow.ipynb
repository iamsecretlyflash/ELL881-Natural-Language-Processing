{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(tokenizer, token_data, span_start, span_end):\n",
    "    \n",
    "    mapList = []\n",
    "    tokenizedList = []\n",
    "    maxLength = 200\n",
    "    for datapoint in token_data:\n",
    "        \n",
    "        curMap = {}\n",
    "        curTokenList = []\n",
    "        for i in range(len(datapoint)):\n",
    "            curMap[i] = []\n",
    "            curTk = tokenizer.tokenize(datapoint[i])  \n",
    "            #iterate over subwords \n",
    "            for tk in curTk:\n",
    "                curMap[i].append(len(curTokenList))\n",
    "                curTokenList.append(tokenizer.convert_tokens_to_ids(tk))\n",
    "        curTokenList.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "\n",
    "        mapList.append(curMap)\n",
    "        tokenizedList.append(curTokenList)\n",
    "    \n",
    "    #padding \n",
    "    attn_mask = np.zeros((len(token_data), maxLength))\n",
    "    for i in range(len(tokenizedList)):\n",
    "        for j in range(len(tokenizedList[i])):\n",
    "            attn_mask[i][j] = 1\n",
    "        while (len(tokenizedList[i]) < maxLength):\n",
    "            tokenizedList[i].append(tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "\n",
    "    #y value - [pad - 0, O - 1, I - 2, B - 3]\n",
    "    y_val = np.ones((len(token_data), maxLength))\n",
    "    for dp in range(len(span_start)):\n",
    "        #overwrite points in ranges 'I'\n",
    "        for i in range(len(span_start[dp])):\n",
    "            for idx in range(span_start[dp][i], span_end[dp][i]+1):\n",
    "                for k in mapList[dp][idx]:           \n",
    "                    if (idx == span_start[dp][i]):\n",
    "                        y_val[dp][k] = 3 #B\n",
    "                    else:\n",
    "                        y_val[dp][k] = 2 #I\n",
    "        \n",
    "    # mark the padded sequence as 'Not part of tweet': 3\n",
    "    for i in range(len(token_data)):\n",
    "        for j in range(maxLength):\n",
    "            if (attn_mask[i][j] == 0):\n",
    "                \n",
    "                if (y_val[i][j] != 1):\n",
    "                    print(\"Assertion_Failed\")\n",
    "\n",
    "                y_val[i][j] = 0\n",
    "\n",
    "    return np.array(tokenizedList), attn_mask, y_val, mapList\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getIOData(fileName: str,\n",
    "              definition_path: str,\n",
    "              tokenizer):\n",
    "    data = pd.read_csv(fileName)\n",
    "    token_data, span_start, span_end = [], [], []\n",
    "    cnt_nonClaims = 0\n",
    "    for i in range(len(data)):\n",
    "        if (data['claim_label'][i] == 1):\n",
    "            token_data.append(ast.literal_eval((data['tokens'][i])))\n",
    "            span_start.append(json.loads(data['span_start_index'][i]))\n",
    "            span_end.append(json.loads(data['span_end_index'][i]))\n",
    "        elif (data['claim_label'][i] == 0):\n",
    "            cnt_nonClaims += 1\n",
    "    tokenizedList, attn_mask, y_val, mapList = getData(tokenizer, token_data, span_start, span_end)\n",
    "    model_inputs = {}\n",
    "    model_inputs['input_ids'] = torch.tensor([i for i in tokenizedList], dtype=torch.long, device=DEVICE)\n",
    "    model_inputs['attention_mask'] = torch.tensor([i for i in attn_mask], dtype=torch.long, device=DEVICE)\n",
    "    model_inputs['labels'] = torch.tensor([i for i in y_val], dtype=torch.long, device=DEVICE)\n",
    "    model_inputs['definition_inputs'] = torch.unsqueeze(pd.read_pickle(definition_path), 0).repeat(model_inputs['input_ids'].shape[0], 1, 1, 1)\n",
    "\n",
    "    return model_inputs, mapList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaam = getIOData('train.csv', 'claim_desc_orig_vecs.pkl', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä to']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>claim_label</th>\n",
       "      <th>span_start_index</th>\n",
       "      <th>span_end_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['\"who', ' may', ' (or', ' may', ' not', ') ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[43]</td>\n",
       "      <td>[53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['RT', ' @Coach_Brod', ': If', ' you', ' have'...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['#Pharmacists', ' warn', ' against', ' #malar...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['You', ' got', ' to', ' boil', ' your', ' Clo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 22]</td>\n",
       "      <td>[20, 33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['There', ' is', ' no', ' virus', '. \\nAnd', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  claim_label  \\\n",
       "0  ['\"who', ' may', ' (or', ' may', ' not', ') ha...            1   \n",
       "1  ['RT', ' @Coach_Brod', ': If', ' you', ' have'...            1   \n",
       "2  ['#Pharmacists', ' warn', ' against', ' #malar...            1   \n",
       "3  ['You', ' got', ' to', ' boil', ' your', ' Clo...            1   \n",
       "4  ['There', ' is', ' no', ' virus', '. \\nAnd', '...            1   \n",
       "\n",
       "  span_start_index span_end_index  \n",
       "0             [43]           [53]  \n",
       "1              [2]           [17]  \n",
       "2              [0]            [4]  \n",
       "3          [0, 22]       [20, 33]  \n",
       "4              [0]            [3]  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['#P\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' if',\n",
       " ' we',\n",
       " ' have',\n",
       " ' not',\n",
       " ' had',\n",
       " ' the',\n",
       " ' virus',\n",
       " ', we',\n",
       " ' are',\n",
       " ' not',\n",
       " ' immune']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(in1)[43:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "label_map = {'<pad>':0, 'O':1, 'I':2, 'B':3}\n",
    "for i in range(len(df)):\n",
    "    text = ast.literal_eval(df['tokens'][i])\n",
    "    texts.append(text)\n",
    "    spans = [(i,j) for i,j in zip(ast.literal_eval(df['span_start_index'][i]), ast.literal_eval(df['span_end_index'][i]))]\n",
    "    label = ['O']*len(text)\n",
    "    for span in spans:\n",
    "        label[span[0]] = 'B'\n",
    "        for i in range(span[0]+1, span[1]+1):\n",
    "            label[i] = 'I'\n",
    "    labels.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6044, 6044)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "label_map = {'<pad>':0, 'O':1, 'I':2, 'B':3}\n",
    "for i in range(len(df)):\n",
    "    text = ast.literal_eval(df['tokens'][i])\n",
    "    texts.append(text)\n",
    "    spans = [(i,j) for i,j in zip(ast.literal_eval(df['span_start_index'][i]), ast.literal_eval(df['span_end_index'][i]))]\n",
    "    label = ['O']*len(text)\n",
    "    for span in spans:\n",
    "        label[span[0]] = 'B'\n",
    "        for i in range(span[0]+1, span[1]+1):\n",
    "            label[i] = 'I'\n",
    "    labels.append(label)\n",
    "\n",
    "tokens = []\n",
    "labels_final = []\n",
    "map_list = []\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    text = texts[i]\n",
    "    tokens_list = []\n",
    "    labels_list = []\n",
    "    cur_map = {}\n",
    "    tokens_list.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "    for j in range(len(text)):\n",
    "        tokenized = tokenizer.tokenize(text[j])\n",
    "        cur_map[j] = []\n",
    "        for sub_token in tokenized:\n",
    "            cur_map[j].append(len(tokens_list))\n",
    "            tokens_list.append(tokenizer.convert_tokens_to_ids(sub_token))\n",
    "    tokens_list.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "\n",
    "    for k in range(len(text)):\n",
    "        for j in cur_map[k]:\n",
    "            labels_list.append(labels[i][k])\n",
    "    labels_final.append(labels_list)\n",
    "    tokens.append(tokens_list)\n",
    "    map_list.append(cur_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " '\"',\n",
       " 'who',\n",
       " 'Ä may',\n",
       " 'Ä (',\n",
       " 'or',\n",
       " 'Ä may',\n",
       " 'Ä not',\n",
       " ')',\n",
       " 'Ä have',\n",
       " 'Ä it',\n",
       " '\"',\n",
       " 'Ä -',\n",
       " 'Ä Sch',\n",
       " 'rod',\n",
       " 'inger',\n",
       " \"'s\",\n",
       " 'Ä Virus',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Ä We',\n",
       " 'Ä can',\n",
       " 'Ä not',\n",
       " 'Ä get',\n",
       " 'Ä tested',\n",
       " ',',\n",
       " 'Ä so',\n",
       " 'Ä we',\n",
       " 'Ä have',\n",
       " 'Ä to',\n",
       " 'Ä act',\n",
       " 'Ä like',\n",
       " 'Ä we',\n",
       " 'Ä have',\n",
       " 'Ä the',\n",
       " 'Ä virus',\n",
       " 'Ä so',\n",
       " 'Ä we',\n",
       " 'Ä do',\n",
       " 'Ä not',\n",
       " 'Ä spread',\n",
       " 'Ä it',\n",
       " '.',\n",
       " 'Ä ',\n",
       " 'Ä We',\n",
       " 'Ä also',\n",
       " 'Ä have',\n",
       " 'Ä to',\n",
       " 'Ä act',\n",
       " 'Ä like',\n",
       " 'Ä we',\n",
       " 'Ä have',\n",
       " 'Ä never',\n",
       " 'Ä had',\n",
       " 'Ä the',\n",
       " 'Ä virus',\n",
       " ',',\n",
       " 'Ä because',\n",
       " 'Ä if',\n",
       " 'Ä we',\n",
       " 'Ä have',\n",
       " 'Ä not',\n",
       " 'Ä had',\n",
       " 'Ä the',\n",
       " 'Ä virus',\n",
       " ',',\n",
       " 'Ä we',\n",
       " 'Ä are',\n",
       " 'Ä not',\n",
       " 'Ä immune',\n",
       " '.#',\n",
       " 'CO',\n",
       " 'VID',\n",
       " '19',\n",
       " '</s>']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945d65c791c743258c3dcdaf95698e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3084e7d5ba7d4acd8f9e19b2b9b90c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d32511b3ff4c2692e1b6990246191d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c5279905e443c1bb7e37a0199a7d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â¦)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "btokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ä unused\n"
     ]
    }
   ],
   "source": [
    "for wrd in tokenizer.vocab.keys():\n",
    "    if 'unused' in wrd:\n",
    "        print(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23797"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['Ä unused']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(tokenizer, token_data, span_start, span_end):\n",
    "    \n",
    "    mapList = []\n",
    "    tokenizedList = []\n",
    "    maxLength = 200\n",
    "    for datapoint in token_data:\n",
    "        \n",
    "        curMap = {}\n",
    "        curTokenList = [tokenizer.convert_tokens_to_ids(tokenizer.cls_token)]\n",
    "        for i in range(len(datapoint)):\n",
    "            curMap[i] = []\n",
    "            curTk = tokenizer.tokenize(datapoint[i])  \n",
    "            #iterate over subwords \n",
    "            for tk in curTk:\n",
    "                curMap[i].append(len(curTokenList))\n",
    "                curTokenList.append(tokenizer.convert_tokens_to_ids(tk))\n",
    "        curTokenList.append(tokenizer.convert_tokens_to_ids(tokenizer.sep_token))\n",
    "\n",
    "        mapList.append(curMap)\n",
    "        tokenizedList.append(curTokenList)\n",
    "    \n",
    "    #padding \n",
    "    attn_mask = np.zeros((len(token_data), maxLength))\n",
    "    for i in range(len(tokenizedList)):\n",
    "        for j in range(len(tokenizedList[i])):\n",
    "            attn_mask[i][j] = 1\n",
    "        while (len(tokenizedList[i]) < maxLength):\n",
    "            tokenizedList[i].append(tokenizer.convert_tokens_to_ids(tokenizer.pad_token))\n",
    "\n",
    "    #y value - [pad - 0, O - 1, I - 2, B - 3]\n",
    "    y_val = np.ones((len(token_data), maxLength))\n",
    "    for dp in range(len(span_start)):\n",
    "        #overwrite points in ranges 'I'\n",
    "        for i in range(len(span_start[dp])):\n",
    "            for idx in range(span_start[dp][i], span_end[dp][i]+1):\n",
    "                for k in mapList[dp][idx]:           \n",
    "                    if (idx == span_start[dp][i]):\n",
    "                        y_val[dp][k] = 3 #B\n",
    "                    else:\n",
    "                        y_val[dp][k] = 2 #I\n",
    "        \n",
    "    # mark the padded sequence as 'Not part of tweet': 3\n",
    "    for i in range(len(token_data)):\n",
    "        for j in range(maxLength):\n",
    "            if (attn_mask[i][j] == 0):\n",
    "                \n",
    "                if (y_val[i][j] != 1):\n",
    "                    print(\"Assertion_Failed\")\n",
    "\n",
    "                y_val[i][j] = 0\n",
    "\n",
    "    return np.array(tokenizedList), attn_mask, y_val, mapList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtok = RobertaTokenizerFast.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'anya']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtok.tokenize('sanya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
